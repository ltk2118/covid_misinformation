# -*- coding: utf-8 -*-
"""AI Modelshare Competition Code

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/ltk2118/covid_misinformation/blob/portfolio_updates/AI_Modelshare_Competition_Code.ipynb

# **COVID Misinformation AI Modelshare Competition**

This model playground can be used to generate predictions for Covid related text. When the text indicates Covid misinformation the model powering the web-dashboard and prediction REST api returns "fake" when the text indicates real Covid related information the model returns "real".

https://www.modelshare.org/detail/model:804#

## Connect to GPU/TPU
"""

gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Not connected to a GPU')
else:
  print(gpu_info)

#install aimodelshare library
! pip install aimodelshare --upgrade
! pip install transformers

# Get competition data
from aimodelshare import download_data
import aimodelshare as ai
download_data('public.ecr.aws/y2e2a1d6/covid_tweet_competition_data-repository:latest')

"""## Splitting"""

# Separate data into X_train, y_train, and X_test
import pandas as pd
import numpy as np

X_train=pd.read_csv("covid_tweet_competition_data/X_train.csv", squeeze=True)
X_test=pd.read_csv("covid_tweet_competition_data/X_test.csv", squeeze=True)

y_train_labels=pd.read_csv("covid_tweet_competition_data/y_train_labels.csv", squeeze=True)

# OH encode y data
y_train = pd.get_dummies(y_train_labels)

"""# EDA"""

#Assess structure of the training data
X_train.shape, y_train.shape

from matplotlib import pyplot as plt

df = pd.DataFrame({"Tweet":X_train, "Label_real":y_train['real']})
fig,(ax1,ax2)=plt.subplots(1,2,figsize=(12,8))
text_len=df[df['Label_real']==1]['Tweet'].str.len()
ax1.hist(text_len,color='red')
ax1.set_title('Real text')
text_len=df[df['Label_real']==0]['Tweet'].str.len()
ax2.hist(text_len,color='green')
ax2.set_title('Fake text')
fig.suptitle('Characters in texts')
plt.show()

# A lot of fake tweets are very long but real tweets seem to be shorter
print(sum(df[df['Label_real']==0]['Tweet'].str.len()>400))
print(sum(df[df['Label_real']==0]['Tweet'].str.len()>1000))
print(sum(df[df['Label_real']==0]['Tweet'].str.len()>2500))
print(sum(df[df['Label_real']==1]['Tweet'].str.len()>400))
print(sum(df[df['Label_real']==1]['Tweet'].str.len()>1000))
print(sum(df[df['Label_real']==1]['Tweet'].str.len()>2500))

# A lot of fake tweets are very long but real tweets seem to be shorter
print(sum(df[df['Label_real']==1]['Tweet'].str.len()<10))
print(sum(df[df['Label_real']==0]['Tweet'].str.len()<10))

"""# Packages for building model"""

# Import libraries for the model training
import tensorflow as tf
import re, string, unicodedata
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv1D, MaxPool1D, Dropout, Input
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix

#BERT
import transformers
from transformers import TFBertModel
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import ModelCheckpoint
from tqdm.notebook import tqdm
from tokenizers import BertWordPieceTokenizer
from transformers import BertTokenizer

"""# Mount Drive"""

from google.colab import drive
drive.mount("/content/drive")

"""# **BERT** Model
Bidirectional Encoder Representations from Transformers (BERT) is a transformer-based machine learning technique for natural language processing (NLP) pre-training developed by Google. BERT was created and published in 2018 by Jacob Devlin and his colleagues from Google

References: https://www.kaggle.com/andreshg/nlp-glove-bert-tf-idf-lstm-explained

# Initialize BERT Tokenizer
"""

tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')

def bert_encode(data, maximum_length) :
    input_ids = []
    attention_masks = []

    for text in data:
        encoded = tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=maximum_length,
            pad_to_max_length=True,

            return_attention_mask=True,
        )
        input_ids.append(encoded['input_ids'])
        attention_masks.append(encoded['attention_mask'])

    return np.array(input_ids),np.array(attention_masks)

"""# Define BERT Model Builder"""

def create_model(bert_model):

    input_ids = tf.keras.Input(shape=(310,),dtype='int32')
    attention_masks = tf.keras.Input(shape=(310,),dtype='int32')

    output = bert_model([input_ids,attention_masks])
    output = output[1]
    output = tf.keras.layers.Dense(64,activation='relu')(output)
    output = tf.keras.layers.Dropout(0.2)(output)
    output = tf.keras.layers.Dense(1,activation='sigmoid')(output)

    model = tf.keras.models.Model(inputs = [input_ids,attention_masks],outputs = output)
    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])
    return model

#load pre-trained uncased BERT
bert_model = TFBertModel.from_pretrained('bert-base-uncased')

"""# Create (Instantiate) BERT Model"""

model_BERT_loss1e5_epoch5 = create_model(bert_model)
model_BERT_loss1e5_epoch5.summary()

"""# Encode X_train in BERT format"""

train_input_ids, train_attention_masks = bert_encode(X_train, 310)

"""# Fit BERT Model, 5 epochs, batch size 16, learning rate 1e-5"""

history = model_BERT_loss1e5_epoch5.fit(
    [train_input_ids, train_attention_masks],
    y_train['real'],
    epochs=5,
    batch_size=16
)

"""# Generate BERT Predictions"""

test_input_ids, test_attention_masks = bert_encode(X_test, 310)

predictions = model_BERT_loss1e5_epoch5.predict((test_input_ids, test_attention_masks))

"""# Export BERT Predictions"""

prediction_labels = [y_train.columns[1] if x > 0.5 else y_train.columns[0] for x in predictions]
prediction_labels_df = pd.DataFrame({"labels":prediction_labels,"tweet":X_test})

from google.colab import drive
drive.mount("/content/drive")
prediction_labels_df.to_csv("/content/drive/MyDrive/Colab Notebooks/bert_preds_loss1e5_310f2.csv")

"""# Save Model and Weights"""

! pip install -U tf2onnx

import tf2onnx
import onnx

model_proto, external_tensor_storage = tf2onnx.convert.from_keras(model_BERT_loss1e5_epoch5, output_path)
onnx.save(model_proto, "/content/drive/MyDrive/Colab Notebooks/final_model.onnx")
model_BERT_loss1e5_epoch5.save_weights("/content/drive/MyDrive/Colab Notebooks/bert_weights")